# =============================================================================
# AI ASSISTANT SERVICE - ENVIRONMENT CONFIGURATION
# =============================================================================

# -----------------------------------------------------------------------------
# ------------------------------ Application ----------------------------------
# -----------------------------------------------------------------------------

APP_NAME="AI Assistant Service"
APP_VERSION="0.1.0"
ENVIRONMENT="development"  # development, production, testing
DEBUG=true
LOG_LEVEL="INFO"  # Debug, Info, Warning, Error, Critical
HOST="0.0.0.0"
PORT=8000
RELOAD=true
SERVICE_PERMISSIONS_PATH="app/config/permissions_map.json"

# -----------------------------------------------------------------------------
# -------------------------- Database - PostgreSQL ----------------------------
# -----------------------------------------------------------------------------

DATABASE_URL="postgresql://ai_assistant:ai_assistant_password@localhost:5432/ai_assistant_db"
DB_ECHO=false  # Set to true to see SQL queries
DB_POOL_SIZE=5
DB_MAX_OVERFLOW=10

# -----------------------------------------------------------------------------
# --------------------------- LLM Configuration -------------------------------
# -----------------------------------------------------------------------------

# Provider: ollama (local), openai, anthropic
LLM_PROVIDER="ollama"

LLM_BASE_URL="http://localhost:11434"  # Ollama: http://localhost:11434 | OpenAI: https://api.openai.com/v1 | Anthropic: https://api.anthropic.com
LLM_API_KEY=""  # Not needed for Ollama
LLM_MODEL="qwen3:8b"  # Ollama: deepseek-r1:7b | OpenAI: gpt-4 | Anthropic: claude-3-opus
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000
LLM_TIMEOUT=120

# Agent Configuration
AGENT_TYPE="structured-chat"  # structured-chat, openai-functions
MAX_AGENT_ITERATIONS=10
AGENT_VERBOSE=true
ENABLE_STREAMING=true

# Chat Configuration
LLM_MAX_CONTEXT_LENGTH=32000
DEFAULT_TOKENS_PER_CHAR=0.35
MESSAGE_OVERHEAD_TOKENS=4

# Tools Configuration
TOOLS_DIRECTORY="./tools"
ENABLE_DYNAMIC_TOOLS=true
TOOLS_TIMEOUT=30

# -----------------------------------------------------------------------------
# -------------------------- ChromaDB (Vector Database) -----------------------
# -----------------------------------------------------------------------------

CHROMA_MODE="local"  # 'local' or 'server'

# server mode
CHROMA_HOST="localhost"  
CHROMA_PORT=8001

# local mode
CHROMA_PERSIST_DIRECTORY="./data/chroma"

# Embeddings
EMBEDDING_MODEL="all-MiniLM-L6-v2"  # sentence-transformers model
EMBEDDING_DIMENSION=384
EMBEDDING_DEVICE="cpu"  # cpu or cuda

# Document Processing
DOCUMENTS_STORAGE_PATH="./data/documents"
MAX_UPLOAD_SIZE_MB=50
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
SEPARATORS=["\n\n", "\n", " ", ""]

# Retrieval Configuration
TOP_K_RETRIEVAL=5  # Number of chunks to retrieve
BM25_WEIGHT=0.3  # Weight for BM25 search (0-1)
VECTOR_WEIGHT=0.7  # Weight for vector search (0-1)
MIN_RELEVANCE_SCORE=0.5  # Minimum similarity score

# -----------------------------------------------------------------------------
# ----------------- Authentication - IAM Service Integration
# -----------------------------------------------------------------------------

# Set to true to enable IAM integration, false for public mode
AUTH_ENABLED=false 

JWT_SECRET_KEY="your-secret-key-change-this-in-production"
JWT_ALGORITHM="HS256"
ACCESS_TOKEN_EXPIRE_MINUTES=30

IAM_SERVICE_URL="http://localhost:8000"
IAM_SERVICE_VERSION="v1"
IAM_SERVICE_TIMEOUT=10

# IAM Service AI Client Authentication for using tools
IAM_CLIENT_ID="your-client-id" 
IAM_CLIENT_SECRET="your-client-secret"

# -----------------------------------------------------------------------------
# ------------------------------ MLflow (MLOps) -------------------------------
# -----------------------------------------------------------------------------

MLFLOW_TRACKING_URI="http://localhost:5000"
MLFLOW_EXPERIMENT_NAME="ai-assistant-experiments"
MLFLOW_ENABLE_TRACKING=true

# -----------------------------------------------------------------------------
# --------------------------------- Redis -------------------------------------
# -----------------------------------------------------------------------------

REDIS_URL="redis://localhost:6379/0"
REDIS_ENABLED=false

# -----------------------------------------------------------------------------
# -------------------------------- Celery -------------------------------------
# -----------------------------------------------------------------------------

CELERY_BROKER_URL="redis://localhost:6379/1"
CELERY_RESULT_BACKEND="redis://localhost:6379/2"
CELERY_ENABLED=false

# -----------------------------------------------------------------------------
# ---------------------------------- CORS -------------------------------------
# -----------------------------------------------------------------------------

CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080"]
CORS_ALLOW_CREDENTIALS=true
CORS_ALLOW_METHODS=["*"]
CORS_ALLOW_HEADERS=["*"]

# -----------------------------------------------------------------------------
# ------------------------------ Rate Limiting -------------------------------
# -----------------------------------------------------------------------------

RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100  # Requests per minute
RATE_LIMIT_PERIOD=60  # Period in seconds

# -----------------------------------------------------------------------------
# --------------------------- Monitoring & Logging ----------------------------
# -----------------------------------------------------------------------------

LOG_FORMAT="json"  # json or text
LOG_FILE="./logs/app.log"
LOG_ROTATION="10 MB"
LOG_RETENTION="30 days"
LOG_PRIVACY_LEVEL="standard"  # none, standard, strict

# -----------------------------------------------------------------------------
# ------------------------------ Feature Flags -------------------------------
# -----------------------------------------------------------------------------

ENABLE_RAG=true
ENABLE_TOOLS=true
ENABLE_MULTI_TURN=true
ENABLE_FEEDBACK=true

# -----------------------------------------------------------------------------
# ------------------------------ Performance -------------------------------
# -----------------------------------------------------------------------------

WORKERS=1  # Number of Uvicorn workers (for production)
BATCH_SIZE=32  # For embedding generation
MAX_CONCURRENT_REQUESTS=100
